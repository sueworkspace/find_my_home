# [Find My Home] Sprint 3 회고 — 크롤링 2.5시간을 5분으로 줄인 이야기

## 프로젝트 소개

**Find My Home**은 서울 주요 구(강남/서초/송파)의 아파트 급매물을 탐지하는 개인 프로젝트입니다.

네이버 부동산 호가와 KB시세를 비교해서 시세 대비 저렴한 매물을 자동으로 찾아줍니다.

기술 스택: Python 3.9 + FastAPI + React 18 + PostgreSQL

---

## Sprint 3 목표

> 전체 크롤링 2.5시간+ 문제를 해결하자.

Sprint 2에서 네이버 크롤러, KB시세, 실거래가, 가격비교 엔진, 프론트엔드까지 한 스프린트에 다 만들었습니다. 기능은 돌아갔지만 **성능 문제**가 터졌습니다.

3개 구 기준 **4,555건 API 호출**, **114분 소요**. 60분 간격 스케줄러에 넣으면 끝나기도 전에 다음 사이클이 시작되는 구조였습니다.

---

## 문제 분석

기존 크롤링 흐름을 뜯어보니 비효율이 명확했습니다.

```
[기존] 매 60분마다:
  1. 네이버 전체 크롤링 (4555 API 호출, 114분)
  2. KB시세 수집
  3. 가격 비교
  → 끝나기 전에 다음 사이클 시작
```

실제로 매물이 바뀌는 단지는 소수인데, **매번 모든 단지의 모든 매물을 다시 긁고 있었습니다.**

추가로 발견한 낭비:
- 200세대 미만 소규모 단지도 전부 크롤링 (실질적으로 급매 분석 대상 아님)
- 매물 0건인 단지도 상세 API를 호출
- KB시세는 하루에 한 번이면 충분한데 매 사이클마다 호출

---

## 해결: 증분 크롤링 아키텍처

### 핵심 아이디어

네이버 API의 단지 목록 응답에 `dealCnt`(매매 매물 수)가 포함되어 있습니다. 이걸 DB의 활성 매물 수와 비교하면, **실제로 변화가 있는 단지만 골라낼 수 있습니다.**

### 4단계 증분 로직

```
Phase 1: 동별 단지 목록 수집 (~55 API 호출, cheap)
Phase 2: 200세대 미만 → 스킵
Phase 3: dealCnt == 0 → API 호출 없이 DB 매물 일괄 비활성화
Phase 4: dealCnt != DB 활성 매물 수 → 이 단지만 상세 크롤링
```

Phase 1은 항상 실행합니다. 단지 목록을 모르면 변화를 감지할 수 없으니까요. 하지만 이건 가벼운 호출이라 55건 정도밖에 안 됩니다.

Phase 2~4에서 대부분의 단지가 걸러지고, **실제 상세 크롤링은 100~200건 수준**으로 줄어듭니다.

### 스케줄러 3분할

데이터 특성에 맞게 수집 주기도 분리했습니다.

| 잡 | 주기 | 이유 |
|----|------|------|
| 네이버 크롤링 + 가격 비교 | 150분 간격 | 매물 변화 반영 주기 |
| KB시세 | 매일 06:00 | KB시세는 하루 1회 갱신 |
| 실거래가 | 매일 02:00 | 공공데이터 일 1회 업데이트 |

### 첫 실행 자동 감지

DB가 비어있으면 전체 크롤링, 데이터가 있으면 증분 크롤링으로 자동 분기합니다. 별도 플래그 관리 없이 `ApartmentComplex` 테이블의 row 수만 보면 됩니다.

```python
def _is_first_run() -> bool:
    count = db.query(ApartmentComplex).count()
    return count == 0
```

---

## 결과

### 성능 개선

| 지표 | Before | After | 개선 |
|------|--------|-------|------|
| API 호출 수 | 4,555건 | ~200건 | **20배 감소** |
| 소요 시간 | 114분 | ~5분 | **22배 단축** |
| 스케줄러 간격 | 60분 (불가능) | 150분 (여유) | 정상 운영 가능 |

### 코드 변경

3개 파일만 수정했습니다:

- `config/settings.py` — 설정 4개 추가
- `naver_crawler.py` — 증분 메서드 추가 (기존 코드 유지)
- `scheduler.py` — 3개 독립 잡으로 재구성

기존 `crawl_region`, `crawl_all_target_regions`는 그대로 두고 `crawl_region_incremental`을 추가하는 방식으로 구현해서, 롤백 리스크가 없습니다.

---

## 삽질 기록: 실거래가 0건의 비밀

Sprint 3 작업 중에 실거래가가 0건인 걸 발견했습니다. API 호출은 성공하는데 저장이 안 되는 상황.

원인: **DB에 단지가 20개뿐**이어서 실거래가 API의 아파트명과 매칭이 안 되고 있었습니다.

```
API: "래미안블레스티지", "개포주공5단지", "삼성동힐스테이트"
DB:  "대치삼성3차", "대치르엘", "래미안대치하이스턴"
→ 매칭률 3%
```

6개월치(2025.08~2026.01)를 수동으로 돌려서 34건을 확보했지만, 근본 해결은 전체 크롤링을 돌려서 단지 수를 200개+로 늘리는 것입니다. 이건 Sprint 4에서 합니다.

---

## 회고

### 잘한 것
- 기존 코드를 건드리지 않고 증분 로직을 추가해서 안전하게 확장
- `dealCnt` 필드를 활용한 변화 감지가 핵심 — API 응답을 잘 뜯어본 게 주효
- 스케줄러를 데이터 특성에 맞게 분리한 건 운영 관점에서 좋은 판단

### 아쉬운 것
- 데이터 확보를 미뤘음 — 20개 단지로는 검증이 제한적
- 배포가 Sprint 2부터 계속 밀리고 있음 (Sprint 4에서는 반드시)
- 미커밋 코드 12개 파일이 쌓여있음

### Sprint 4 계획
1. 전체 크롤링 1회 실행 (3개 구 200+ 단지 확보)
2. 미커밋 코드 정리
3. Docker + EC2 배포
4. 실거래가 매칭 개선

---

*Find My Home은 개인 학습 목적의 프로젝트입니다. 네이버 부동산, KB부동산 API는 비상업적 용도로 사용하며, rate limiting을 준수합니다.*
